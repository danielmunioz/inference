name: scrape
on:
  workflow_dispatch:
    inputs:
      library:
        description: (optional) Python name of a library to scrape (edit libraries_requirements.txt first)
        type: string
        required: false
      scrape-artifact-run-id:
        description: (optional) Run ID of the scrape artifact to download and continue scraping from
        type: number
        required: false
  schedule:
    # At 00:00 on the 1st day of every 3rd month.
    - cron: '0 0 1 */3 *'
jobs:
  looping:
    runs-on: ubuntu-latest
    outputs:
      matrix: ${{steps.set-matrix.outputs.result}}
    steps:
      - id: set-matrix  # maximum is 256 jobs
        run: echo "result=[$(seq -s ', ' 1 140)]" >> "$GITHUB_OUTPUT"

  scraping:
    needs: [looping]
    runs-on:
      labels: ubuntu-latest-8-cores
    strategy:
      max-parallel: 1  # TODO: parallelize jobs
      matrix:
        node: ${{fromJSON(needs.looping.outputs.matrix)}}
    steps:
      - name: Checkout üõéÔ∏è graph-compiler
        uses: actions/checkout@v2
        with:
          path: graph-compiler
          persist-credentials: false
          submodules: 'recursive'

      - name: Download previous workflow scrape artifact
        uses: dawidd6/action-download-artifact@v2
        if: matrix.node == 1 && inputs.scrape-artifact-run-id == ''
        continue-on-error: true
        with:
          github_token: ${{ secrets.IVY_LEAVES_TOKEN }}
          workflow: scrape.yml
          workflow_conclusion: ""
          check_artifacts: true
          name: scrape-artifacts
          path: graph-compiler/lib_scraping/scrape/result

      - name: Download specific workflow scrape artifact
        uses: dawidd6/action-download-artifact@v2
        if: matrix.node == 1 && inputs.scrape-artifact-run-id != ''
        continue-on-error: true
        with:
          github_token: ${{ secrets.IVY_LEAVES_TOKEN }}
          run_id: ${{ inputs.scrape-artifact-run-id }}
          name: scrape-artifacts
          path: graph-compiler/lib_scraping/scrape/result

      - name: Download continuous scrape artifacts
        if: matrix.node != 1
        uses: actions/download-artifact@v3
        with:
          name: scrape-artifacts
          path: graph-compiler/lib_scraping/scrape/result

      - name: Scrape
        uses: xoiga123/retry-max@v2.0.1
        with:
          max_timeout_minutes: 60
          timeout_minutes: 999
          max_attempts: 999
          retry_on: error
          retry_wait_seconds: 60
          continue_on_error: true
          command: |
            cd "${{github.workspace}}/graph-compiler"
            if [[ ${{ matrix.node }} == 1 || ! -f "lib_scraping/scrape/result/all_done" ]]; then
              docker run --rm -m 25g --init -v `pwd`:/ivy/graph-compiler unifyai/ivy:latest bash /ivy/graph-compiler/lib_scraping/scrape/scrape.sh ${{ matrix.node }} ${{ inputs.library }}
            else
              echo "Scrape is complete, skipping subsequent jobs to complete workflow"
            fi

      - name: Upload scrape artifacts
        uses: actions/upload-artifact@v3
        if: always()
        with:
          name: scrape-artifacts
          path: graph-compiler/lib_scraping/scrape/result
          retention-days: 365
